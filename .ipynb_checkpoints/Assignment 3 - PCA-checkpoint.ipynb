{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment #3\n",
    "\n",
    "### Técnicas de Agrupamento\n",
    "\n",
    "Pedro Stramantinoli P. Cagliume Gomes 175955\n",
    "\n",
    "Ruy Castilho Barrichelo 177012\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_health():\n",
    "    txt_path = os.path.join(\"health-dataset\", \"health.txt\")\n",
    "    with open(txt_path) as f:\n",
    "        data = f.readlines()[1:]\n",
    "    parsed = []\n",
    "    for line in data:\n",
    "        parsed.append(line.replace('\\n', '').split('|'))\n",
    "    return np.array(parsed)\n",
    "\n",
    "\n",
    "def load_bags():\n",
    "    csv_path = os.path.join(\"health-dataset\", \"bags.csv\")\n",
    "    return pd.read_csv(csv_path, header=None)\n",
    "\n",
    "def load_word2vec():\n",
    "    csv_path = os.path.join(\"health-dataset\", \"word2vec.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Bag of Words\n",
    "bag = np.array(load_bags())\n",
    "processed = StandardScaler().fit_transform(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca1 = PCA(n_components=0.99,svd_solver='full')\n",
    "fitted1 = pca1.fit_transform(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca2 = PCA(n_components=0.98,svd_solver='full')\n",
    "fitted2 = pca2.fit_transform(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca3 = PCA(n_components=0.95,svd_solver='full')\n",
    "fitted3 = pca3.fit_transform(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca4 = PCA(n_components=0.90,svd_solver='full')\n",
    "fitted4 = pca4.fit_transform(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca5 = PCA(n_components=0.85,svd_solver='full')\n",
    "fitted5 = pca5.fit_transform(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca6 = PCA(n_components=0.50,svd_solver='full')\n",
    "fitted6 = pca6.fit_transform(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13229, 1165)\n",
      "(13229, 1138)\n",
      "(13229, 1070)\n",
      "(13229, 973)\n",
      "(13229, 887)\n",
      "(13229, 427)\n"
     ]
    }
   ],
   "source": [
    "print(fitted1.shape)\n",
    "print(fitted2.shape)\n",
    "print(fitted3.shape)\n",
    "print(fitted4.shape)\n",
    "print(fitted5.shape)\n",
    "print(fitted6.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances, euclidean_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def silhouette(data, labels, metric):\n",
    "    return silhouette_score(data, labels, metric=metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calinski Harabaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabaz_score\n",
    "\n",
    "def calinski_harabaz(data, labels):\n",
    "    return calinski_harabaz_score(data, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "MAX_ITER = 300\n",
    "N_INIT = 1\n",
    "\n",
    "results_kmeans = []\n",
    "\n",
    "fitted = [fitted1, fitted2, fitted3, fitted4, fitted5, fitted6]\n",
    "kmeans = KMeans(n_clusters = k, init = 'random', max_iter=MAX_ITER, n_jobs=-1, random_state=21)\n",
    "\n",
    "for index in range(len(fitted)):\n",
    "    data = fitted[index]\n",
    "    kmeans_fit = kmeans.fit(data) \n",
    "    silhouette_score_kmeans = silhouette(data, kmeans.labels_, 'euclidean')\n",
    "    calinski_score_kmeans = calinski_harabaz(data, kmeans.labels_)\n",
    "\n",
    "    results_kmeans.append((index, kmeans_fit, silhouette_score_kmeans, calinski_score_kmeans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "      n_clusters=278, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "      random_state=21, tol=0.0001, verbose=0), -0.043158523942383696, 12.95067871887165),\n",
       " (1, KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "      n_clusters=278, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "      random_state=21, tol=0.0001, verbose=0), -0.04111833593325869, 13.180191994153422),\n",
       " (2, KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "      n_clusters=278, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "      random_state=21, tol=0.0001, verbose=0), -0.035992626206292004, 13.686166248578632),\n",
       " (3, KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "      n_clusters=278, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "      random_state=21, tol=0.0001, verbose=0), -0.03392648045170416, 14.560192425326138),\n",
       " (4, KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "      n_clusters=278, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "      random_state=21, tol=0.0001, verbose=0), -0.02997109992336919, 15.418505745628918),\n",
       " (5, KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "      n_clusters=278, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "      random_state=21, tol=0.0001, verbose=0), 0.0795494314705765, 25.42255042041058)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_kmeanss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances, euclidean_distances\n",
    "import numpy as np\n",
    "from scipy import sparse, spatial\n",
    "from math import log\n",
    "\n",
    "def dbscanClustering(data, eps, min_samples, metric):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, n_jobs=-1)\n",
    "\n",
    "    return dbscan.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "metric = 'cosine'\n",
    "eps = 0.6\n",
    "min_samples = 5\n",
    "\n",
    "fitted = [fitted1, fitted2, fitted3, fitted4, fitted5, fitted6]\n",
    "\n",
    "for index in range(len(fitted)):\n",
    "#     bag = fitted[index]\n",
    "    clustered = dbscanClustering(bag, eps, min_samples, metric)\n",
    "    print(len(np.unique(clustered.labels_)))\n",
    "    if ( len(np.unique(clustered.labels_)) > 1 and len(np.unique(clustered.labels_)) <= len(bag) -1 ):\n",
    "        silhouette_score_dbscan = silhouette(bag, clustered.labels_, metric)\n",
    "        calinski_score_dbscan = calinski_harabaz(bag, clustered.labels_)\n",
    "\n",
    "        results.append((index+1, silhouette_score_dbscan, calinski_score_dbscan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'cosine'\n",
    "eps = 0.6\n",
    "min_samples = 5\n",
    "\n",
    "clustered = dbscanClustering(bag, eps, min_samples, metric)\n",
    "silhouette_score_dbscan = silhouette(bag, clustered.labels_, metric)\n",
    "calinski_score_dbscan = calinski_harabaz(bag, clustered.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['359368837147271169' 'Mon Jul 22 17:46:22 +0000 2013'\n",
      " 'Software architect reprograms his diet -- and loses 140 lbs'] ['395631437916807168' 'Wed Oct 30 19:20:59 +0000 2013'\n",
      " 'Am I a bad parent if I give my child candy for #Halloween?']\n"
     ]
    }
   ],
   "source": [
    "tweets = load_health()\n",
    "tweet1 = tweets[2191]\n",
    "tweet2 = tweets[1872]\n",
    "print(tweet1, tweet2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
